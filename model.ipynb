{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 23:41:18.957048: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/electric/anaconda3/envs/ASL_SA/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-16 23:41:18.957084: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from math import inf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import json\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Extracting videos of signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the json file\n",
    "json_file = 'WLASL_v0.3.json'\n",
    "if os.path.exists(json_file) == False:\n",
    "    raise('The json file does not exist!')\n",
    "with open(json_file) as f:\n",
    "    signs = json.load(f)\n",
    "\n",
    "#Check the original videos dir exits\n",
    "old_dir = 'videos'\n",
    "if os.path.exists(old_dir) == False:\n",
    "    raise('The original videos directory does not exist!')\n",
    "    \n",
    "#create the labeled parent dir if it does not exist\n",
    "new_dir = 'labeledVids'\n",
    "if os.path.exists(new_dir) == False:\n",
    "    os.mkdir(new_dir)\n",
    "\n",
    "#loop thru different signs\n",
    "missing_vids = 0\n",
    "for sign in signs:\n",
    "\n",
    "    #extract sign name\n",
    "    sign_name = sign['gloss']\n",
    "\n",
    "    #Directory of the sign\n",
    "    sign_dir = os.path.join(new_dir, sign_name)\n",
    "\n",
    "    #Create a subdir of sign_name under the new videos directory, if it does not exist\n",
    "    if os.path.exists(os.path.join(new_dir, sign_name)) == False:\n",
    "        os.mkdir(os.path.join(new_dir, sign_name))\n",
    "\n",
    "    #loop thru vids and copy them from old_dir/ to new/sign_name/\n",
    "    for vid in sign['instances']:\n",
    "\n",
    "        #video old and new paths\n",
    "        vid_name = vid['video_id'] + '.mp4'\n",
    "        vid_old_path = os.path.join(old_dir, vid_name)\n",
    "        vid_new_path = os.path.join(new_dir, sign_name, vid_name)\n",
    "\n",
    "        #check if video exists in old path\n",
    "        if os.path.exists(vid_old_path) == False:\n",
    "            missing_vids += 1\n",
    "            continue\n",
    "\n",
    "        #check if video exists in the new path\n",
    "        if os.path.exists(vid_new_path):\n",
    "            continue\n",
    "\n",
    "        #Add the video to the new dir if it exists in the old and not in the new\n",
    "        shutil.copyfile(vid_old_path, vid_new_path)\n",
    "\n",
    "#remove signs that have less than 4 videos\n",
    "for sign in os.listdir(new_dir):\n",
    "    sign_dir = os.path.join(new_dir, sign)\n",
    "    if len(os.listdir(sign_dir)) < 4:\n",
    "        os.rmdir(sign_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Statistics about data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum number of frames a video has = 16 frames\n",
      "The maximum number of frames a video has = 195 frames\n",
      "\n",
      "Number of videos containing 20 to 40 frames = 537\n",
      "Number of videos containing 25 to 35 frames = 320\n"
     ]
    }
   ],
   "source": [
    "mn, mx = inf, -inf\n",
    "f20t40 = 0\n",
    "f25t35 = 0\n",
    "totalVids = 0\n",
    "frames = []\n",
    "\n",
    "# loop over every sign.\n",
    "for sign in os.listdir(new_dir):\n",
    "\n",
    "    # directory of the sign in the local os.\n",
    "    sign_dir = os.path.join(new_dir, sign)\n",
    "\n",
    "    # loop over every video for a given sign.\n",
    "    for vid in os.listdir(sign_dir):\n",
    "\n",
    "        # path of the vid.\n",
    "        vid_path = os.path.join(sign_dir, vid)\n",
    "\n",
    "        # open video resource.\n",
    "        cap = cv2.VideoCapture(vid_path)\n",
    "\n",
    "        # if opened successfully.\n",
    "        if cap.isOpened():\n",
    "\n",
    "            # increment number of total videos.\n",
    "            totalVids += 1\n",
    "\n",
    "            # min and max number of frames.\n",
    "            mn = min(mn, int(cap. get(cv2. CAP_PROP_FRAME_COUNT)))\n",
    "            mx = max(mx, int(cap. get(cv2. CAP_PROP_FRAME_COUNT)))\n",
    "\n",
    "            # counting number of frames in a given range.\n",
    "            if 20<=cap.get(cv2. CAP_PROP_FRAME_COUNT) and cap.get(cv2. CAP_PROP_FRAME_COUNT)<=40: f20t40 += 1\n",
    "            if 25<=cap.get(cv2. CAP_PROP_FRAME_COUNT) and cap.get(cv2. CAP_PROP_FRAME_COUNT)<=35: f25t35 += 1\n",
    "\n",
    "            # frames distribution of all videos.\n",
    "            frames.append(cap.get(cv2. CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # release the video resource.\n",
    "        cap.release()\n",
    "\n",
    "# display results.\n",
    "print(f'The minimum number of frames a video has = {mn} frames')\n",
    "print(f'The maximum number of frames a video has = {mx} frames')\n",
    "print('')\n",
    "print(f'Number of videos containing 20 to 40 frames = {f20t40}')\n",
    "print(f'Number of videos containing 25 to 35 frames = {f25t35}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extracting Keypoints using mediapipe Holistic\n",
    "The goal here is to extract and save keypoints for later use. This saves computation time during the LSTM model as it won't have to extract the keypoints for each frame itself."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Mediapipe Holistic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. Keypoints extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x557f661ced80] Invalid NAL unit size (745 > 472).\n",
      "[h264 @ 0x557f661ced80] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x557f6657c8c0] stream 1, offset 0x3b468: partial file\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x557f6657c8c0] stream 1, offset 0x3b7d3: partial file\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x557f6657c8c0] stream 1, offset 0x3c9b9: partial file\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x557f6657c8c0] stream 1, offset 0x3cd00: partial file\n",
      "[h264 @ 0x557f65bcf580] Invalid NAL unit size (71678 > 10776).\n",
      "[h264 @ 0x557f65bcf580] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x557f6653d6c0] stream 1, offset 0x2a27a7: partial file\n"
     ]
    }
   ],
   "source": [
    "# directory from which to read videos of signs.\n",
    "signs_dir = 'labeledVids'\n",
    "if os.path.exists(signs_dir) == False:\n",
    "    raise('Directory of videos does not exist!')\n",
    "\n",
    "# directory to save the extracted keypoints\n",
    "kps_dir = 'labeledKeypoints'\n",
    "if os.path.exists(kps_dir) == False:\n",
    "    os.mkdir(kps_dir)\n",
    "\n",
    "# set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # for every available sign\n",
    "    for sign in os.listdir(signs_dir):\n",
    "\n",
    "        # directory of the current sign\n",
    "        sign_dir = os.path.join(signs_dir, sign)\n",
    "\n",
    "        # keypoints directory of the sign\n",
    "        kp_sign_dir = os.path.join(kps_dir, sign)\n",
    "        if os.path.exists(kp_sign_dir) == False:\n",
    "            os.mkdir(kp_sign_dir)\n",
    "\n",
    "\n",
    "        # for every video sequence in the sign directory\n",
    "        for vid in os.listdir(sign_dir):\n",
    "\n",
    "            # path of the vid relative to the parent directory of signs_dir\n",
    "            vid_path = os.path.join(sign_dir, vid)\n",
    "\n",
    "            # keypoints directory of the vid\n",
    "            kp_vid_dir = os.path.join(kp_sign_dir, vid)\n",
    "            if os.path.exists(kp_vid_dir) == False:\n",
    "                os.mkdir(kp_vid_dir)\n",
    "            \n",
    "            # capturing the video\n",
    "            cap = cv2.VideoCapture(vid_path)\n",
    "\n",
    "            if cap.isOpened() == False:\n",
    "                raise('could not capture video')\n",
    "\n",
    "            # counter of the frames\n",
    "            frame_num = 0\n",
    "\n",
    "            # for each frame in the video\n",
    "            while cap.isOpened():\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "                if ret == False:\n",
    "                    break\n",
    "\n",
    "                # increment frame number\n",
    "                frame_num += 1\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                \n",
    "                # save keypoints to save computation time in the future.\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(kp_vid_dir, str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "            \n",
    "            # release the video capture\n",
    "            cap.release()\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The LSTM model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding of signs\n",
    "signs_dir = 'labeledKeypoints2'\n",
    "signs = os.listdir(signs_dir)\n",
    "sign_map = {sign:num for num, sign in enumerate(signs)}\n",
    "\n",
    "# generating inputs\n",
    "vids, labels = [], []\n",
    "for sign in os.listdir(signs_dir):\n",
    "    \n",
    "    sign_dir = os.path.join(signs_dir, sign)\n",
    "\n",
    "    for vid in os.listdir(sign_dir):\n",
    "        window = []\n",
    "        vid_dir = os.path.join(sign_dir, vid)\n",
    "\n",
    "        for frame in os.listdir(vid_dir):\n",
    "            frame_path = os.path.join(vid_dir, frame)\n",
    "            res = np.load(frame_path)\n",
    "            window.append(res)\n",
    "\n",
    "        vids.append(window)\n",
    "        labels.append(sign_map[sign])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(vids, dtype='float')\n",
    "y = to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 23:41:46.124918: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-12-16 23:41:46.124951: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-12-16 23:41:46.133201: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-12-16 23:41:46.185973: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-12-16 23:41:46.197261: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-12-16 23:41:46.212916: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-12-16 23:41:46.212948: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (predator): /proc/driver/nvidia/version does not exist\n",
      "2022-12-16 23:41:46.213651: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-16 23:41:46.214312: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "#for saving logs\n",
    "log_dir = os.path.join('logs')\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.mkdir(log_dir)\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "# model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(None,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(len(sign_map), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, None, 64)          442112    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 128)         98816     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 108)               3564      \n",
      "=================================================================\n",
      "Total params: 600,140\n",
      "Trainable params: 600,140\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 23:41:56.598279: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 882462168 exceeds 10% of free system memory.\n",
      "2022-12-16 23:41:57.106504: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-12-16 23:41:57.131429: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199995000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 1/29 [>.............................] - ETA: 1:14 - loss: 4.6756 - categorical_accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 23:41:59.921614: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-12-16 23:41:59.921665: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/29 [=>............................] - ETA: 9s - loss: 16.1799 - categorical_accuracy: 0.0000e+00 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 23:42:00.206582: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "2022-12-16 23:42:00.306279: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-12-16 23:42:00.455009: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/train/plugins/profile/2022_12_16_23_42_00\n",
      "2022-12-16 23:42:00.540196: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to logs/train/plugins/profile/2022_12_16_23_42_00/predator.trace.json.gz\n",
      "2022-12-16 23:42:00.589829: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/train/plugins/profile/2022_12_16_23_42_00\n",
      "2022-12-16 23:42:00.589933: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to logs/train/plugins/profile/2022_12_16_23_42_00/predator.memory_profile.json.gz\n",
      "2022-12-16 23:42:00.591336: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/train/plugins/profile/2022_12_16_23_42_00Dumped tool data for xplane.pb to logs/train/plugins/profile/2022_12_16_23_42_00/predator.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/train/plugins/profile/2022_12_16_23_42_00/predator.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/train/plugins/profile/2022_12_16_23_42_00/predator.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/train/plugins/profile/2022_12_16_23_42_00/predator.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/train/plugins/profile/2022_12_16_23_42_00/predator.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 10s 277ms/step - loss: nan - categorical_accuracy: 0.0078\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 7s 252ms/step - loss: nan - categorical_accuracy: 0.0078\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 7s 236ms/step - loss: nan - categorical_accuracy: 0.0085\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 7s 241ms/step - loss: nan - categorical_accuracy: 0.0109\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 8s 263ms/step - loss: nan - categorical_accuracy: 0.0093\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 9s 303ms/step - loss: nan - categorical_accuracy: 0.0067\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 7s 252ms/step - loss: nan - categorical_accuracy: 0.0076\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 7s 249ms/step - loss: nan - categorical_accuracy: 0.0079\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 7s 242ms/step - loss: nan - categorical_accuracy: 0.0106\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 7s 238ms/step - loss: nan - categorical_accuracy: 0.0085\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 8s 286ms/step - loss: nan - categorical_accuracy: 0.0090\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 9s 307ms/step - loss: nan - categorical_accuracy: 0.0109\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 8s 282ms/step - loss: nan - categorical_accuracy: 0.0060\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 7s 238ms/step - loss: nan - categorical_accuracy: 0.0072\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 7s 236ms/step - loss: nan - categorical_accuracy: 0.0103\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 7s 240ms/step - loss: nan - categorical_accuracy: 0.0105\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 7s 231ms/step - loss: nan - categorical_accuracy: 0.0090\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 7s 224ms/step - loss: nan - categorical_accuracy: 0.0110\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 7s 248ms/step - loss: nan - categorical_accuracy: 0.0061\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 9s 306ms/step - loss: nan - categorical_accuracy: 0.0110\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 7s 233ms/step - loss: nan - categorical_accuracy: 0.0072\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 7s 241ms/step - loss: nan - categorical_accuracy: 0.0068\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 7s 248ms/step - loss: nan - categorical_accuracy: 0.0116\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 8s 293ms/step - loss: nan - categorical_accuracy: 0.0121\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 8s 250ms/step - loss: nan - categorical_accuracy: 0.0093\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 6s 222ms/step - loss: nan - categorical_accuracy: 0.0066\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 6s 220ms/step - loss: nan - categorical_accuracy: 0.0127\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: nan - categorical_accuracy: 0.0071\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 7s 241ms/step - loss: nan - categorical_accuracy: 0.0105\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 7s 238ms/step - loss: nan - categorical_accuracy: 0.0086\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 8s 275ms/step - loss: nan - categorical_accuracy: 0.0089\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 7s 249ms/step - loss: nan - categorical_accuracy: 0.0091\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 9s 297ms/step - loss: nan - categorical_accuracy: 0.0134\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 7s 234ms/step - loss: nan - categorical_accuracy: 0.0047\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 7s 233ms/step - loss: nan - categorical_accuracy: 0.0095\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 7s 236ms/step - loss: nan - categorical_accuracy: 0.0149\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 7s 235ms/step - loss: nan - categorical_accuracy: 0.0084\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 7s 258ms/step - loss: nan - categorical_accuracy: 0.0091\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 9s 296ms/step - loss: nan - categorical_accuracy: 0.0143\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 7s 247ms/step - loss: nan - categorical_accuracy: 0.0133\n",
      "Epoch 41/100\n",
      "29/29 [==============================] - 7s 225ms/step - loss: nan - categorical_accuracy: 0.0168\n",
      "Epoch 42/100\n",
      "29/29 [==============================] - 9s 310ms/step - loss: nan - categorical_accuracy: 0.0172\n",
      "Epoch 43/100\n",
      "29/29 [==============================] - 6s 220ms/step - loss: nan - categorical_accuracy: 0.0068\n",
      "Epoch 44/100\n",
      "29/29 [==============================] - 7s 224ms/step - loss: nan - categorical_accuracy: 0.0161\n",
      "Epoch 45/100\n",
      "29/29 [==============================] - 6s 221ms/step - loss: nan - categorical_accuracy: 0.0064\n",
      "Epoch 46/100\n",
      "29/29 [==============================] - 7s 246ms/step - loss: nan - categorical_accuracy: 0.0095\n",
      "Epoch 47/100\n",
      "29/29 [==============================] - 8s 257ms/step - loss: nan - categorical_accuracy: 0.0078\n",
      "Epoch 48/100\n",
      "29/29 [==============================] - 7s 232ms/step - loss: nan - categorical_accuracy: 0.0170\n",
      "Epoch 49/100\n",
      "29/29 [==============================] - 7s 235ms/step - loss: nan - categorical_accuracy: 0.0102\n",
      "Epoch 50/100\n",
      "29/29 [==============================] - 7s 236ms/step - loss: nan - categorical_accuracy: 0.0077\n",
      "Epoch 51/100\n",
      "29/29 [==============================] - 7s 233ms/step - loss: nan - categorical_accuracy: 0.0076\n",
      "Epoch 52/100\n",
      "29/29 [==============================] - 7s 235ms/step - loss: nan - categorical_accuracy: 0.0145\n",
      "Epoch 53/100\n",
      "29/29 [==============================] - 8s 280ms/step - loss: nan - categorical_accuracy: 0.0110\n",
      "Epoch 54/100\n",
      "29/29 [==============================] - 8s 282ms/step - loss: nan - categorical_accuracy: 0.0074\n",
      "Epoch 55/100\n",
      "29/29 [==============================] - 10s 360ms/step - loss: nan - categorical_accuracy: 0.0122\n",
      "Epoch 56/100\n",
      "29/29 [==============================] - 8s 273ms/step - loss: nan - categorical_accuracy: 0.0080\n",
      "Epoch 57/100\n",
      "29/29 [==============================] - 8s 259ms/step - loss: nan - categorical_accuracy: 0.0064\n",
      "Epoch 58/100\n",
      "29/29 [==============================] - 7s 232ms/step - loss: nan - categorical_accuracy: 0.0084\n",
      "Epoch 59/100\n",
      "29/29 [==============================] - 7s 227ms/step - loss: nan - categorical_accuracy: 0.0093\n",
      "Epoch 60/100\n",
      "29/29 [==============================] - 7s 229ms/step - loss: nan - categorical_accuracy: 0.0109\n",
      "Epoch 61/100\n",
      "29/29 [==============================] - 10s 345ms/step - loss: nan - categorical_accuracy: 0.0090\n",
      "Epoch 62/100\n",
      "29/29 [==============================] - 9s 283ms/step - loss: nan - categorical_accuracy: 0.0111\n",
      "Epoch 63/100\n",
      "29/29 [==============================] - 8s 263ms/step - loss: nan - categorical_accuracy: 0.0122\n",
      "Epoch 64/100\n",
      "29/29 [==============================] - 7s 251ms/step - loss: nan - categorical_accuracy: 0.0099\n",
      "Epoch 65/100\n",
      "29/29 [==============================] - 10s 339ms/step - loss: nan - categorical_accuracy: 0.0093\n",
      "Epoch 66/100\n",
      "29/29 [==============================] - 7s 228ms/step - loss: nan - categorical_accuracy: 0.0093\n",
      "Epoch 67/100\n",
      "29/29 [==============================] - 6s 220ms/step - loss: nan - categorical_accuracy: 0.0110\n",
      "Epoch 68/100\n",
      "29/29 [==============================] - 6s 221ms/step - loss: nan - categorical_accuracy: 0.0083\n",
      "Epoch 69/100\n",
      "29/29 [==============================] - 9s 303ms/step - loss: nan - categorical_accuracy: 0.0063\n",
      "Epoch 70/100\n",
      "29/29 [==============================] - 7s 236ms/step - loss: nan - categorical_accuracy: 0.0108\n",
      "Epoch 71/100\n",
      "29/29 [==============================] - 7s 244ms/step - loss: nan - categorical_accuracy: 0.0054\n",
      "Epoch 72/100\n",
      "29/29 [==============================] - 7s 235ms/step - loss: nan - categorical_accuracy: 0.0100\n",
      "Epoch 73/100\n",
      "29/29 [==============================] - 8s 285ms/step - loss: nan - categorical_accuracy: 0.0155\n",
      "Epoch 74/100\n",
      "29/29 [==============================] - 8s 251ms/step - loss: nan - categorical_accuracy: 0.0085\n",
      "Epoch 75/100\n",
      "29/29 [==============================] - 7s 242ms/step - loss: nan - categorical_accuracy: 0.0135\n",
      "Epoch 76/100\n",
      "29/29 [==============================] - 7s 223ms/step - loss: nan - categorical_accuracy: 0.0107\n",
      "Epoch 77/100\n",
      "29/29 [==============================] - 8s 283ms/step - loss: nan - categorical_accuracy: 0.0085\n",
      "Epoch 78/100\n",
      "29/29 [==============================] - 6s 216ms/step - loss: nan - categorical_accuracy: 0.0093\n",
      "Epoch 79/100\n",
      "29/29 [==============================] - 6s 223ms/step - loss: nan - categorical_accuracy: 0.0151\n",
      "Epoch 80/100\n",
      "29/29 [==============================] - 7s 240ms/step - loss: nan - categorical_accuracy: 0.0085\n",
      "Epoch 81/100\n",
      "29/29 [==============================] - 7s 233ms/step - loss: nan - categorical_accuracy: 0.0058\n",
      "Epoch 82/100\n",
      "29/29 [==============================] - 6s 213ms/step - loss: nan - categorical_accuracy: 0.0081\n",
      "Epoch 83/100\n",
      "29/29 [==============================] - 6s 214ms/step - loss: nan - categorical_accuracy: 0.0053\n",
      "Epoch 84/100\n",
      "29/29 [==============================] - 6s 216ms/step - loss: nan - categorical_accuracy: 0.0109\n",
      "Epoch 85/100\n",
      "29/29 [==============================] - 6s 214ms/step - loss: nan - categorical_accuracy: 0.0094\n",
      "Epoch 86/100\n",
      "29/29 [==============================] - 6s 222ms/step - loss: nan - categorical_accuracy: 0.0069\n",
      "Epoch 87/100\n",
      "29/29 [==============================] - 7s 235ms/step - loss: nan - categorical_accuracy: 0.0048\n",
      "Epoch 88/100\n",
      "29/29 [==============================] - 7s 232ms/step - loss: nan - categorical_accuracy: 0.0163\n",
      "Epoch 89/100\n",
      "29/29 [==============================] - 7s 233ms/step - loss: nan - categorical_accuracy: 0.0137\n",
      "Epoch 90/100\n",
      "29/29 [==============================] - 7s 232ms/step - loss: nan - categorical_accuracy: 0.0139\n",
      "Epoch 91/100\n",
      "29/29 [==============================] - 8s 294ms/step - loss: nan - categorical_accuracy: 0.0079\n",
      "Epoch 92/100\n",
      "29/29 [==============================] - 7s 253ms/step - loss: nan - categorical_accuracy: 0.0079\n",
      "Epoch 93/100\n",
      "29/29 [==============================] - 7s 237ms/step - loss: nan - categorical_accuracy: 0.0137\n",
      "Epoch 94/100\n",
      "29/29 [==============================] - 7s 232ms/step - loss: nan - categorical_accuracy: 0.0120\n",
      "Epoch 95/100\n",
      "29/29 [==============================] - 7s 237ms/step - loss: nan - categorical_accuracy: 0.0138\n",
      "Epoch 96/100\n",
      "29/29 [==============================] - 7s 237ms/step - loss: nan - categorical_accuracy: 0.0075\n",
      "Epoch 97/100\n",
      "29/29 [==============================] - 7s 234ms/step - loss: nan - categorical_accuracy: 0.0099\n",
      "Epoch 98/100\n",
      "29/29 [==============================] - 9s 295ms/step - loss: nan - categorical_accuracy: 0.0083\n",
      "Epoch 99/100\n",
      "29/29 [==============================] - 7s 241ms/step - loss: nan - categorical_accuracy: 0.0113\n",
      "Epoch 100/100\n",
      "29/29 [==============================] - 9s 302ms/step - loss: nan - categorical_accuracy: 0.0099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f32e848dfd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.fit(X_train, y_train, epochs=100, callbacks=[tb_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(X_test)\n",
    "y_true = np.argmax(y_test, axis=1).tolist()\n",
    "y_predicted = np.argmax(y_predicted, axis=1).tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0., 48.],\n",
       "        [ 0.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[45.,  0.],\n",
       "        [ 3.,  0.]],\n",
       "\n",
       "       [[46.,  0.],\n",
       "        [ 2.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[46.,  0.],\n",
       "        [ 2.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[45.,  0.],\n",
       "        [ 3.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[46.,  0.],\n",
       "        [ 2.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[46.,  0.],\n",
       "        [ 2.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[47.,  0.],\n",
       "        [ 1.,  0.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(y_true, y_predicted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ASL_SA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a18f16a49242d2f63b70d92dab1caded852c317a519656dd3ad2c2ce380ac0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
